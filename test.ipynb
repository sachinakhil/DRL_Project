{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport logging\nimport numpy as np\nimport random\nfrom collections import deque\nimport sys\nimport contextlib\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\n\n# Suppress TensorFlow logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ntf.get_logger().setLevel('ERROR')\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Test logging output\nlogging.debug('Debug logging is working.')\nlogging.info('Info logging is working.')\nprint('Print statements are working.')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EygXkCN_o0N","outputId":"f96afd06-308a-4ebc-baba-72fd403e0082","execution":{"iopub.status.busy":"2024-10-21T03:18:36.114275Z","iopub.execute_input":"2024-10-21T03:18:36.114702Z","iopub.status.idle":"2024-10-21T03:18:49.353924Z","shell.execute_reply.started":"2024-10-21T03:18:36.114662Z","shell.execute_reply":"2024-10-21T03:18:49.352876Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Print statements are working.\n","output_type":"stream"}]},{"cell_type":"code","source":"from gym import spaces\n\nclass TrafficEnv:\n    def __init__(self):\n        # Traffic light states: 1 for green, 0 for red\n        self.signal_NS = 1  # Initially green for NS\n        self.signal_EW = 0  # Initially red for EW\n        self.cars_NS = np.random.randint(0, 10)  # Cars waiting in NS direction\n        self.cars_EW = np.random.randint(0, 10)  # Cars waiting in EW direction\n\n        # Observation space includes the number of cars and traffic light states\n        self.observation_space = spaces.Box(\n            low=np.array([0, 0, 0, 0, 0]), \n            high=np.array([np.inf, np.inf, 1, 1, np.inf]), \n            dtype=np.float32\n        )\n\n        # Action space: each agent can choose to keep or switch the signal\n        self.action_space = spaces.MultiDiscrete([2, 2])  # Actions for NS and EW signals\n\n        self.time_since_last_switch = 0\n        self.state = self.get_state()\n\n    def get_state(self):\n        # State includes number of cars and signal states\n        return np.array([\n            self.cars_NS, \n            self.cars_EW, \n            self.signal_NS, \n            self.signal_EW, \n            self.time_since_last_switch\n        ])\n\n    def reset(self):\n        self.cars_NS = np.random.randint(0, 10)\n        self.cars_EW = np.random.randint(0, 10)\n        self.signal_NS = 1\n        self.signal_EW = 0\n        self.time_since_last_switch = 0\n        self.state = self.get_state()\n        return self.state\n\n    def step(self, action):\n        reward = 0\n        self.time_since_last_switch += 1\n\n        # Unpack the action for both directions\n        action_NS, action_EW = action\n\n        # Update traffic lights based on actions\n        self.signal_NS = action_NS\n        self.signal_EW = action_EW\n\n        # Update car movements based on current signals\n        cars_passed_NS = 0\n        cars_passed_EW = 0\n        if self.signal_NS == 1:\n            cars_passed_NS = min(self.cars_NS, np.random.randint(1, 3))\n            self.cars_NS -= cars_passed_NS\n            reward += cars_passed_NS\n        if self.signal_EW == 1:\n            cars_passed_EW = min(self.cars_EW, np.random.randint(1, 3))\n            self.cars_EW -= cars_passed_EW\n            reward += cars_passed_EW\n\n        # New cars arrive at the intersection\n        self.cars_NS += np.random.randint(0, 3)\n        self.cars_EW += np.random.randint(0, 3)\n\n        # Update state\n        self.state = self.get_state()\n        done = self.time_since_last_switch >= 100  # End the episode after 100 steps\n\n        return self.state, reward, done, {}\n\n    def update_traffic_lights(self, action, direction):\n        # Deprecated method; actions are now handled in step()\n        pass\n","metadata":{"id":"8qNefhKz_rek","execution":{"iopub.status.busy":"2024-10-21T03:18:54.214436Z","iopub.execute_input":"2024-10-21T03:18:54.215115Z","iopub.status.idle":"2024-10-21T03:18:54.547085Z","shell.execute_reply.started":"2024-10-21T03:18:54.215071Z","shell.execute_reply":"2024-10-21T03:18:54.546098Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@contextlib.contextmanager\ndef suppress_stdout():\n    with open(os.devnull, \"w\") as devnull:\n        old_stdout = sys.stdout\n        sys.stdout = devnull\n        try:\n            yield\n        finally:\n            sys.stdout = old_stdout\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size  # e.g., 5\n        self.action_size = action_size  # e.g., 2\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95    # Discount rate\n        self.epsilon = 1.0   # Exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n        \n    def _build_model(self):\n        # Build the neural network model\n        model = Sequential()\n        model.add(Input(shape=(self.state_size,)))\n        model.add(Dense(24, activation='relu'))\n        model.add(Dense(24, activation='relu'))\n        model.add(Dense(self.action_size, activation='linear'))\n        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n        return model\n        \n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        with suppress_stdout():\n            act_values = self.model.predict(np.array([state]), verbose=0)\n        return np.argmax(act_values[0])  # Returns action\n        \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n        \n    def train(self, batch_size=32):\n        if len(self.memory) < batch_size:\n            logging.debug('Not enough memory to train.')\n            return\n        minibatch = random.sample(self.memory, batch_size)\n        states = []\n        targets = []\n        for idx, (state, action, reward, next_state, done) in enumerate(minibatch):\n            target = reward\n            if not done:\n                with suppress_stdout():\n                    target += self.gamma * np.amax(self.model.predict(np.array([next_state]), verbose=0)[0])\n            target_f = self.model.predict(np.array([state]), verbose=0)\n            target_f[0][action] = target\n            states.append(state)\n            targets.append(target_f[0])\n            logging.debug(f'Training on sample {idx+1}/{batch_size}')\n        with suppress_stdout():\n            self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n        # Reduce exploration rate\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        # Log training progress\n        logging.debug(f'Agent trained on batch of size {batch_size}. Epsilon: {self.epsilon:.4f}')\n        \n    def load(self, name):\n        self.model.load_weights(name)\n        \n    def save(self, name):\n        self.model.save_weights(name)\n","metadata":{"id":"1ju11cVu_0xK","execution":{"iopub.status.busy":"2024-10-21T03:19:52.184880Z","iopub.execute_input":"2024-10-21T03:19:52.185945Z","iopub.status.idle":"2024-10-21T03:19:52.204553Z","shell.execute_reply.started":"2024-10-21T03:19:52.185879Z","shell.execute_reply":"2024-10-21T03:19:52.203215Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train_multi_agent(env, agent1, agent2, episodes, batch_size=32):\n    for episode in range(episodes):\n        state = env.reset()\n        done = False\n        total_reward = 0\n        step_count = 0\n\n        logging.info(f'Starting Episode {episode+1}/{episodes}')\n\n        while not done:\n            # Agent 1 (NS direction) takes an action\n            action1 = agent1.act(state)\n            # Agent 2 (EW direction) takes an action\n            action2 = agent2.act(state)\n\n            # Both agents' actions are applied to the environment\n            next_state, reward, done, _ = env.step([action1, action2])\n\n            # Each agent gets the same reward and next state (you might want to customize this)\n            agent1.remember(state, action1, reward, next_state, done)\n            agent2.remember(state, action2, reward, next_state, done)\n\n            state = next_state\n            total_reward += reward\n            step_count += 1\n\n            # Train both agents\n            agent1.train(batch_size)\n            agent2.train(batch_size)\n\n            # Log step information every 10 steps\n            if step_count % 10 == 0:\n                logging.debug(f'Episode {episode+1}, Step {step_count}, Total Reward: {total_reward}')\n\n        logging.info(f'Episode {episode+1} completed. Steps: {step_count}, Total Reward: {total_reward}')\n        logging.info(f'Agent1 Epsilon: {agent1.epsilon:.4f}, Agent2 Epsilon: {agent2.epsilon:.4f}')\n\n    # Save trained models\n    agent1.save(\"agent1_model.h5\")\n    agent2.save(\"agent2_model.h5\")\n    logging.info('Training completed and models saved.')\n","metadata":{"id":"G6joobps_6AJ","execution":{"iopub.status.busy":"2024-10-21T03:19:04.758341Z","iopub.execute_input":"2024-10-21T03:19:04.758773Z","iopub.status.idle":"2024-10-21T03:19:04.767739Z","shell.execute_reply.started":"2024-10-21T03:19:04.758730Z","shell.execute_reply":"2024-10-21T03:19:04.766475Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    env = TrafficEnv()\n    state_size = env.observation_space.shape[0]\n    action_size = env.action_space.nvec[0]  # Since action_space is MultiDiscrete\n\n    # Initialize two DQN agents for the two directions\n    agent1 = DQNAgent(state_size, action_size)\n    agent2 = DQNAgent(state_size, action_size)\n\n    # Start training\n    logging.info('Starting training process...')\n    train_multi_agent(env, agent1, agent2, episodes=100)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T03:20:02.574327Z","iopub.execute_input":"2024-10-21T03:20:02.575167Z"},"trusted":true},"execution_count":null,"outputs":[]}]}